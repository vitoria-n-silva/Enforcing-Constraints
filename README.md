# Enforcing-Constraints
Despite progress in techniques that reduce hallucination, LLM-conversational assistants (LLMCAs) struggles to consistently enforce contextual constraints. This limitation arises from the models’ reliance on generalized statistical associations learned during training, which can override user-defined restrictions. For instance, in healthcare, an LLMCA might suggest high-glycemic foods to a diabetic patient, despite explicit instructions to the contrary. We investigate strategies to ensure LLMCAs adhere to contextual constraints, analyzing the effectiveness of five different approaches in mitigating inappropriate responses. Our methodology involves controlled tests where an automated system assesses the probability of constraint violations across 30 critical scenarios. The study quantifies failure incidence for each approach, applying statistical analyses to identify patterns of inadequate responses and evaluate refinement mechanisms' effectiveness. Based on the results, we propose an integrated strategy inspired by Self Taught Evaluators (STE), which outperforms the other methods in enhancing LLMCAs’ compliance with contextual constraints.
